{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msfasha/307307-BI-Methods-LLMs/blob/main/section_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZo4Cyqf_w-B"
      },
      "source": [
        "### **Natural Language Processing (NLP) Basics**\n",
        "\n",
        "#### **1.1 What is NLP and Why Is It Important?**\n",
        "**Definition:**  \n",
        "Natural Language Processing (NLP) is a field of Artificial Intelligence that enables computers to understand, interpret, and generate human language.<br> NLP bridges the gap between human communication and machine understanding, allowing businesses to analyze and leverage text data effectively.\n",
        "\n",
        "**Applications of NLP in Business:**\n",
        "1. **Customer Insights:** Sentiment analysis of customer reviews and feedback.\n",
        "2. **Automation:** Automated chatbots for customer support.\n",
        "3. **Content Generation:** Writing articles, generating reports, or creating marketing content.\n",
        "4. **Decision Support:** Extracting insights from financial reports, contracts, and legal documents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1Of_3xU_w-E"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c3hpLCu_w-E"
      },
      "source": [
        "#### **1.2 Core Concepts in NLP**\n",
        "\n",
        "**1.2.1 Text Preprocessing**  \n",
        "Preprocessing prepares raw text data for analysis by cleaning and standardizing it. This step is critical because raw text contains noise like punctuation, special characters, and inconsistencies.\n",
        "\n",
        "**Steps of Text Preprocessing with Code:**\n",
        "\n",
        "1. **Tokenization:** Breaking text into smaller components, such as words or sentences.\n",
        "2. **Removing Stopwords:** Filtering out common words (e.g., \"is,\" \"the\") that don’t contribute much meaning.\n",
        "3. **Stemming and Lemmatization:** Reducing words to their root form for consistency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Python Code:**\n",
        "\n",
        "First of all, we need to install NLTN, Python main NLP library.<br>\n",
        "The Natural Language Toolkit (NLTK) is a comprehensive library for natural language processing (NLP) in Python. It provides easy-to-use interfaces to over 50 corpora and lexical resources, such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.<br> NLTK also includes wrappers for industrial-strength NLP libraries. It is widely used for research and educational purposes due to its simplicity and extensive documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orFLKmRW_w-E",
        "outputId": "9ee75d3d-93e0-45b4-83de-d1ac9e34ee38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: joblib in /home/me/myenv/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m406.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mMB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from nltk)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm, regex, click, nltk\n",
            "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
          ]
        }
      ],
      "source": [
        "! pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_NPnEwJ_w-K"
      },
      "source": [
        "**Example text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm7vpYRoAT8I",
        "outputId": "ce88de76-a05e-453e-bc0b-3f00fe33fa34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['Natural', 'Language', 'Processing', 'is', 'an', 'exciting', 'field', 'of', 'Artificial', 'Intelligence', '!']\n",
            "Filtered Tokens: ['Natural', 'Language', 'Processing', 'exciting', 'field', 'Artificial', 'Intelligence', '!']\n",
            "Stemmed Tokens: ['natur', 'languag', 'process', 'excit', 'field', 'artifici', 'intellig', '!']\n",
            "Lemmatized Tokens: ['Natural', 'Language', 'Processing', 'exciting', 'field', 'Artificial', 'Intelligence', '!']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /home/me/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/me/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/me/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/me/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab')  # This line is added to download the necessary resource\n",
        "\n",
        "# Download other required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "text = \"Natural Language Processing is an exciting field of Artificial Intelligence!\"\n",
        "\n",
        "# 1. Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# 2. Removing Stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(\"Filtered Tokens:\", filtered_tokens)\n",
        "\n",
        "# 3. Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "\n",
        "# 4. Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYmEwj06_w-N"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8FSQo5Z_w-P"
      },
      "source": [
        "**1.2.2 Bag of Words (BoW) and TF-IDF**  \n",
        "BoW and TF-IDF are techniques to convert text into numerical vectors for machine learning.\n",
        "\n",
        "**Bag of Words:**\n",
        "- Counts the frequency of words in a document.\n",
        "- Doesn't consider the importance or meaning of words.\n",
        "\n",
        "**TF-IDF (Term Frequency-Inverse Document Frequency):**\n",
        "- Assigns weights to words based on their frequency in a document and across all documents.\n",
        "\n",
        "**Python Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Q_me1aYl_w-Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bag of Words Matrix:\n",
            "[[1 0 0 0 0 1 0 0 1 0 0 0]\n",
            " [0 1 0 0 0 1 1 1 0 1 1 0]\n",
            " [0 0 1 1 1 0 1 0 1 0 0 1]]\n",
            "Feature Names: ['amazing' 'future' 'helps' 'human' 'in' 'is' 'language' 'natural' 'nlp'\n",
            " 'processing' 'the' 'understanding']\n",
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.68091856 0.         0.         0.         0.         0.51785612\n",
            "  0.         0.         0.51785612 0.         0.         0.        ]\n",
            " [0.         0.44036207 0.         0.         0.         0.3349067\n",
            "  0.3349067  0.44036207 0.         0.44036207 0.44036207 0.        ]\n",
            " [0.         0.         0.44036207 0.44036207 0.44036207 0.\n",
            "  0.3349067  0.         0.3349067  0.         0.         0.44036207]]\n",
            "Feature Names: ['amazing' 'future' 'helps' 'human' 'in' 'is' 'language' 'natural' 'nlp'\n",
            " 'processing' 'the' 'understanding']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"NLP is amazing.\",\n",
        "    \"Natural Language Processing is the future.\",\n",
        "    \"NLP helps in understanding human language.\"\n",
        "]\n",
        "\n",
        "# Bag of Words\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(documents)\n",
        "print(\"Bag of Words Matrix:\")\n",
        "print(bow_matrix.toarray())\n",
        "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n",
        "print(\"Feature Names:\", tfidf_vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsSx4KzT_w-R"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y0FSSjA_w-R"
      },
      "source": [
        "#### **1.3 Introduction to Vectors**\n",
        "\n",
        "**Definition:**  \n",
        "A **vector** is a mathematical object that has both magnitude (size) and direction. In NLP, vectors are used to represent words, sentences, or documents as points in a multi-dimensional space. Understanding vectors is crucial for understanding word embeddings.\n",
        "\n",
        "**1.3.1 Why Are Vectors Important in NLP?**\n",
        "1. **Representation:** Words and phrases can be represented as numerical vectors, enabling mathematical operations.\n",
        "2. **Similarity:** Vectors help measure similarity between words (e.g., cosine similarity).\n",
        "3. **Operations:** Vectors enable computations like addition, subtraction, and scaling, which are useful for tasks like analogy generation.\n",
        "\n",
        "**1.3.2 Basic Concepts of Vectors**\n",
        "1. **Magnitude:** The length of a vector.\n",
        "2. **Direction:** The orientation of the vector in space.\n",
        "3. **Operations:**\n",
        "   - Addition and subtraction of vectors.\n",
        "   - Dot product (used in cosine similarity).\n",
        "   - Scaling (multiplying a vector by a scalar).\n",
        "\n",
        "**1.3.3 Visualizing Vectors**\n",
        "Vectors can be visualized in 2D or 3D space. For example:\n",
        "- A word like \"king\" might be represented as a vector [0.8, 0.6].\n",
        "- A word like \"queen\" might be represented as [0.7, 0.7].\n",
        "\n",
        "**1.3.4 Practical Python Code for Vectors**\n",
        "\n",
        "**Python Code: Basic Operations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pt0u7qx__w-R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Magnitude of vector_a: 3.605551275463989\n",
            "Sum of vector_a and vector_b: [6 4]\n",
            "Difference of vector_a and vector_b: [-2  2]\n",
            "Dot product of vector_a and vector_b: 11\n",
            "Scaled vector_a: [4 6]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define vectors\n",
        "vector_a = np.array([2, 3])\n",
        "vector_b = np.array([4, 1])\n",
        "\n",
        "# Magnitude of a vector\n",
        "magnitude_a = np.linalg.norm(vector_a)\n",
        "print(\"Magnitude of vector_a:\", magnitude_a)\n",
        "\n",
        "# Addition of vectors\n",
        "vector_sum = vector_a + vector_b\n",
        "print(\"Sum of vector_a and vector_b:\", vector_sum)\n",
        "\n",
        "# Subtraction of vectors\n",
        "vector_diff = vector_a - vector_b\n",
        "print(\"Difference of vector_a and vector_b:\", vector_diff)\n",
        "\n",
        "# Dot product\n",
        "dot_product = np.dot(vector_a, vector_b)\n",
        "print(\"Dot product of vector_a and vector_b:\", dot_product)\n",
        "\n",
        "# Scaling a vector\n",
        "scalar = 2\n",
        "scaled_vector = scalar * vector_a\n",
        "print(\"Scaled vector_a:\", scaled_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jj2gxJ0_w-R"
      },
      "source": [
        "**Python Code: Cosine Similarity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VZ0VcmZD_w-R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine similarity between vector_c and vector_d: 0.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Example vectors\n",
        "vector_c = np.array([1, 0])\n",
        "vector_d = np.array([0, 1])\n",
        "\n",
        "# Reshape vectors to 2D arrays (required for cosine similarity)\n",
        "similarity = cosine_similarity([vector_c], [vector_d])\n",
        "print(\"Cosine similarity between vector_c and vector_d:\", similarity[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO7rizW__w-S"
      },
      "source": [
        "**1.3.5 Key Insights for NLP**\n",
        "1. Vectors allow us to represent words in a way that computers can process.\n",
        "2. Operations like the dot product and cosine similarity enable the measurement of relationships between words.\n",
        "3. Scaling and combining vectors can help derive new relationships (e.g., \"king - man + woman = queen\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z49QwMzB_w-S"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF-5VHnx_w-S"
      },
      "source": [
        "\n",
        "#### **1.4 Introduction to Word Embeddings**\n",
        "Traditional methods like BoW and TF-IDF fail to capture the semantic meaning of words. Word embeddings solve this by representing words as dense numerical vectors in a continuous vector space.\n",
        "\n",
        "**Why Word Embeddings?**\n",
        "- Words with similar meanings have similar vector representations.\n",
        "- Captures relationships like \"king - man + woman = queen.\"\n",
        "\n",
        "**Example: Cosine Similarity of Word Vectors**\n",
        "- Cosine similarity measures how similar two word vectors are in terms of direction.\n",
        "\n",
        "**Python Code:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Fj81Qxi7_w-S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity between 'king' and 'queen': 0.9899494936611666\n",
            "Similarity between 'man' and 'woman': 0.9374252720097653\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Example word embeddings (hypothetical)\n",
        "word_embeddings = {\n",
        "    \"king\": np.array([0.8, 0.6]),\n",
        "    \"queen\": np.array([0.7, 0.7]),\n",
        "    \"man\": np.array([0.5, 0.3]),\n",
        "    \"woman\": np.array([0.4, 0.5])\n",
        "}\n",
        "\n",
        "# Cosine Similarity\n",
        "def calculate_cosine_similarity(word1, word2):\n",
        "    vec1 = word_embeddings[word1]\n",
        "    vec2 = word_embeddings[word2]\n",
        "    similarity = cosine_similarity([vec1], [vec2])\n",
        "    return similarity[0][0]\n",
        "\n",
        "# Example Comparisons\n",
        "print(\"Similarity between 'king' and 'queen':\", calculate_cosine_similarity(\"king\", \"queen\"))\n",
        "print(\"Similarity between 'man' and 'woman':\", calculate_cosine_similarity(\"man\", \"woman\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJxyVtT-_w-S"
      },
      "source": [
        "### **Section 1 Summary**\n",
        "1. NLP bridges the gap between human language and computers.\n",
        "2. Text preprocessing is essential for cleaning and preparing data.\n",
        "3. BoW and TF-IDF are basic text vectorization techniques but lack semantic understanding.\n",
        "4. Word embeddings provide semantic meaning, enabling better NLP applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZvNIVvt_w-S"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2VsLgZq_w-S"
      },
      "source": [
        "### **Section 1.5: Introduction to Arabic Language NLP**\n",
        "\n",
        "#### **1.5.1 Why is Arabic NLP Challenging?**\n",
        "1. **Rich Morphology:** Arabic has complex word structures (e.g., prefixes, suffixes, and infixes).\n",
        "2. **Diacritics:** Words can have different meanings based on diacritics, which are often omitted in text.\n",
        "3. **Word Order:** Arabic has a flexible word order compared to English.\n",
        "4. **Variants:** Multiple dialects exist alongside Modern Standard Arabic (MSA), adding complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK6_bvKS_w-S"
      },
      "source": [
        "#### **1.5.2 Libraries and Tools for Arabic NLP**\n",
        "Here are some libraries that support Arabic:\n",
        "1. **`nltk`:** Basic tokenization and stopword removal.\n",
        "2. **`spacy`:** Tokenization and lemmatization for Arabic.\n",
        "3. **`farasa`:** A tool specifically designed for Arabic NLP tasks like segmentation and diacritization.\n",
        "4. **`pyarabic`:** General utilities for Arabic text processing.\n",
        "5. **`Tashaphyne`:** For stemming Arabic words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hxXbt4z2_w-S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['الذكاء', 'الاصطناعي', 'يساعد', 'في', 'معالجة', 'اللغة', 'العربية', 'بشكل', 'كبير', '.']\n",
            "Filtered Tokens: ['الذكاء', 'الاصطناعي', 'يساعد', 'معالجة', 'اللغة', 'العربية', 'بشكل', 'كبير', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/me/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/me/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download Arabic stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Arabic text example\n",
        "text = \"الذكاء الاصطناعي يساعد في معالجة اللغة العربية بشكل كبير.\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Stopword removal\n",
        "stop_words = set(stopwords.words(\"arabic\"))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "print(\"Filtered Tokens:\", filtered_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxr8XfeJ_w-T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiE8pjEW_w-T"
      },
      "source": [
        "\n",
        "**1.5.3.2 Stemming and Lemmatization (Using `Tashaphyne` and `Spacy`)**\n",
        "\n",
        "**Using `Tashaphyne` for Stemming:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4OQkskO2_w-T"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tashaphyne'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtashaphyne\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstemming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArabicLightStemmer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize the stemmer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m arabic_stemmer \u001b[38;5;241m=\u001b[39m ArabicLightStemmer()\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tashaphyne'"
          ]
        }
      ],
      "source": [
        "from tashaphyne.stemming import ArabicLightStemmer\n",
        "\n",
        "# Initialize the stemmer\n",
        "arabic_stemmer = ArabicLightStemmer()\n",
        "\n",
        "# Example word\n",
        "word = \"المعالجة\"\n",
        "stemmed_word = arabic_stemmer.light_stem(word)\n",
        "print(\"Stemmed Word:\", stemmed_word)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scRfYGEb_w-T"
      },
      "source": [
        "**Using `spacy` for Lemmatization:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-gvYFnx_w-T"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load spacy Arabic model (install with `python -m spacy download ar`)\n",
        "nlp = spacy.load(\"ar_core_news_sm\")\n",
        "\n",
        "# Example text\n",
        "doc = nlp(\"السيارات تسير في الشوارع.\")\n",
        "\n",
        "# Lemmatization\n",
        "lemmatized_words = [token.lemma_ for token in doc]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q77UbELR_w-T"
      },
      "source": [
        "**1.5.3.3 Morphological Analysis and Diacritization (Using `Farasa`)**\n",
        "\n",
        "**Installing Farasa:**\n",
        "Farasa is not available on PyPI and requires downloading its tools from the [Farasa website](https://farasa.qcri.org/). You can use the **`farasa` Python wrapper** if Java is installed.\n",
        "\n",
        "**Farasa Example (Morphological Analysis):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xvsd4k99_w-T"
      },
      "outputs": [],
      "source": [
        "from farasa.segmenter import FarasaSegmenter\n",
        "\n",
        "# Initialize the Farasa Segmenter\n",
        "segmenter = FarasaSegmenter(interactive=True)\n",
        "\n",
        "# Example text\n",
        "text = \"الذكاء الاصطناعي يساعدنا يومياً.\"\n",
        "\n",
        "# Morphological segmentation\n",
        "segmented_text = segmenter.segment(text)\n",
        "print(\"Segmented Text:\", segmented_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTFxdA95_w-U"
      },
      "source": [
        "#### **1.5.4 Common Challenges and Solutions in Arabic NLP**\n",
        "1. **Dialectal Variations:** Use tools like Farasa or train models on specific dialectal corpora.\n",
        "2. **Handling Diacritics:** Use diacritization tools (like Farasa) for disambiguation.\n",
        "3. **Rich Morphology:** Use advanced tokenization and morphological analyzers to handle affixes and infixes.\n",
        "\n",
        "### **1.5.5 Small Arabic NLP Project Idea**\n",
        "**Sentiment Analysis for Arabic Text:**\n",
        "Students can create a simple sentiment analysis tool for Arabic tweets or reviews by:\n",
        "1. Preprocessing the text (tokenization, stopword removal, etc.).\n",
        "2. Using a pre-trained Arabic word embedding (e.g., AraVec).\n",
        "3. Training a classifier like Logistic Regression or a Neural Network.\n",
        "\n",
        "### **Summary of Section 1.5**\n",
        "1. Arabic NLP poses unique challenges due to its morphology, diacritics, and dialects.\n",
        "2. Libraries like `nltk`, `spacy`, and `farasa` are useful for tasks like tokenization, stemming, and diacritization.\n",
        "3. Practical examples help students grasp the differences between English and Arabic NLP processing."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
