{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Natural Language Processing and Classical Language Models\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lecture, students will be able to:\n",
    "1. Understand the basic concepts and challenges of Natural Language Processing\n",
    "2. Explain key text preprocessing techniques\n",
    "3. Describe and implement statistical language models\n",
    "4. Understand n-gram models and their limitations\n",
    "5. Apply basic NLP techniques using Python libraries\n",
    "\n",
    "## 1. Introduction to Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) is a field at the intersection of computer science, artificial intelligence, and linguistics focused on enabling computers to understand, interpret, and generate human language. It forms the foundation for all language technologies we use today, from search engines to voice assistants and modern large language models.\n",
    "\n",
    "### Why is NLP Challenging?\n",
    "\n",
    "Human language is:\n",
    "\n",
    "- **Ambiguous**: Words and sentences can have multiple interpretations\n",
    "  - **Lexical ambiguity**: When a word has multiple meanings\n",
    "    - Example: \"The bank is closed\" (financial institution or riverbank)\n",
    "    - Example: \"The match was struck\" (Small stick used to create fire or Competition between teams)\n",
    "    - Example: \"The suit was expensive\" (Formal set of clothes or Legal proceeding)\n",
    "    - Computational challenge: Systems must select the correct word sense from multiple possibilities\n",
    "  \n",
    "  - **Syntactic ambiguity**: When a sentence can be parsed in multiple ways\n",
    "    - Example: \"I saw the man with the telescope\" (Who has the telescope?)\n",
    "    - Example: \"Flying planes can be dangerous\" (The act of flying planes or planes that are flying?)\n",
    "    - Example: \"Time flies like an arrow\" (Multiple possible grammatical structures)\n",
    "    - Computational challenge: Models must determine the correct grammatical structure\n",
    "  \n",
    "  - **Semantic ambiguity**: When the meaning of a sentence has multiple interpretations\n",
    "    - Example: \"Every student took a different course\" (Each student took one course, and no two students took the same course? Or each student took multiple courses that differed from their own other courses?)\n",
    "    - Example: \"The chicken is ready to eat\" (Ready to be eaten or ready to consume food?)\n",
    "    - Example: \"John and Mary got married last year\" (To each other or to other people?)\n",
    "    - Computational challenge: Systems must infer the intended meaning based on context\n",
    "  \n",
    "  - **Pragmatic ambiguity**: When the intended meaning depends on context, intent, or implied information\n",
    "    - Example: \"It's cold in here\" (Statement of fact or request to close a window/turn up heat?)\n",
    "    - Example: \"Do you know what time it is?\" (Yes/no question or request for the time?)\n",
    "    - Example: \"Could you pass the salt?\" (Question about ability or request for action?)\n",
    "    - Computational challenge: Models must understand communicative intent beyond literal meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Techniques and Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP encompasses a wide range of techniques and applications that work together to process and understand language:\n",
    "\n",
    "#### Fundamental NLP Techniques\n",
    "\n",
    "1. **Tokenization**: Breaking text into words, phrases, or other meaningful elements\n",
    "   - Word tokenization: Splitting \"I love NLP.\" into [\"I\", \"love\", \"NLP\", \".\"]\n",
    "   - Subword tokenization: Breaking words into meaningful pieces (e.g., \"playing\" → [\"play\", \"##ing\"])\n",
    "   - Character tokenization: Splitting text into individual characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-LLMs/main/lecture%20notes/images/tokenization.png\" alt=\"Text Tokenization\" width=\"600\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Part-of-speech (POS) tagging**: Identifying whether words are nouns, verbs, adjectives, etc.\n",
    "   - \"The quick brown fox jumps\" → [Determiner, Adjective, Adjective, Noun, Verb]\n",
    "   - Critical for understanding grammatical structure and word meaning in context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-LLMs/main/lecture%20notes/images/POS-tagging.jpg\" alt=\"Text Tokenization\" width=\"600\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. **Named Entity Recognition (NER)**: Identifying proper nouns like people, organizations, locations\n",
    "   - \"Apple is releasing a new iPhone in San Francisco\" → [Organization, Product, Location]\n",
    "   - Useful for extracting key information from unstructured text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-LLMs/main/lecture%20notes/images/ner.png\" alt=\"Text Tokenization\" width=\"600\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Parsing**: Analyzing grammatical structure of sentences\n",
    "   - Constituency parsing: Constructing phrase structure trees\n",
    "   - Dependency parsing: Identifying grammatical relationships between words\n",
    "   - Semantic parsing: Mapping sentences to formal meaning representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-LLMs/main/lecture%20notes/images/parsing.png\" alt=\"Text Tokenization\" width=\"600\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Coreference resolution**: Determining when different words refer to the same entity\n",
    "   - \"John said he was tired\" → \"he\" refers to \"John\"\n",
    "   - Essential for understanding relationships across sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-LLMs/main/lecture%20notes/images/coreference.png\" alt=\"Text Tokenization\" width=\"600\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common NLP Applications\n",
    "\n",
    "1. **Sentiment analysis**: Determining the emotional tone of text\n",
    "   - Identifying if customer reviews are positive, negative, or neutral\n",
    "   - Tracking public opinion on products, services, or topics\n",
    "\n",
    "2. **Machine translation**: Translating text between languages\n",
    "   - Converting text from one language to another while preserving meaning\n",
    "   - Handling cultural and linguistic differences across languages\n",
    "\n",
    "3. **Text generation**: Creating human-like text\n",
    "   - Automatic summarization of longer documents\n",
    "   - Creative writing assistance and content creation\n",
    "   - Dialogue systems for conversational AI\n",
    "\n",
    "4. **Question answering**: Providing answers to natural language questions\n",
    "   - Extracting answers from documents or knowledge bases\n",
    "   - Understanding user intent and providing relevant information\n",
    "\n",
    "5. **Information extraction**: Identifying and extracting structured information from text\n",
    "   - Pulling key data points from unstructured documents\n",
    "   - Converting text documents into structured databases\n",
    "\n",
    "6. **Text classification**: Categorizing text into predefined categories\n",
    "   - Sorting emails into spam/not spam\n",
    "   - Organizing documents by topic or content type\n",
    "\n",
    "Understanding these techniques and applications provides the foundation for studying language models, which combine these elements to create systems that can comprehend and generate human language at increasingly sophisticated levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n",
    "Before building language models, we need to prepare and clean text data through several preprocessing steps:\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Splitting text into meaningful units (typically words or subwords).\n",
    "\n",
    "```python\n",
    "# Simple word tokenization\n",
    "sentence = \"Large language models are revolutionizing business applications.\"\n",
    "tokens = sentence.split()\n",
    "print(tokens)\n",
    "# Output: ['Large', 'language', 'models', 'are', 'revolutionizing', 'business', 'applications.']\n",
    "\n",
    "# Using NLTK for more sophisticated tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "# Output: ['Large', 'language', 'models', 'are', 'revolutionizing', 'business', 'applications', '.']\n",
    "```\n",
    "\n",
    "### Normalization\n",
    "\n",
    "Converting text to a standard form to reduce variability:\n",
    "\n",
    "```python\n",
    "# Lowercasing\n",
    "normalized_tokens = [token.lower() for token in tokens]\n",
    "print(normalized_tokens)\n",
    "# Output: ['large', 'language', 'models', 'are', 'revolutionizing', 'business', 'applications', '.']\n",
    "\n",
    "# Removing punctuation\n",
    "import re\n",
    "normalized_tokens = [re.sub(r'[^\\w\\s]', '', token.lower()) for token in tokens]\n",
    "print(normalized_tokens)\n",
    "# Output: ['large', 'language', 'models', 'are', 'revolutionizing', 'business', 'applications', '']\n",
    "```\n",
    "\n",
    "### Stopword Removal\n",
    "\n",
    "Eliminating common words that add little meaning:\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in normalized_tokens if token and token not in stop_words]\n",
    "print(filtered_tokens)\n",
    "# Output: ['large', 'language', 'models', 'revolutionizing', 'business', 'applications']\n",
    "```\n",
    "\n",
    "### Stemming and Lemmatization\n",
    "\n",
    "Reducing words to their root forms:\n",
    "\n",
    "```python\n",
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(stemmed_tokens)\n",
    "# Output: ['larg', 'languag', 'model', 'revolution', 'busi', 'applic']\n",
    "\n",
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print(lemmatized_tokens)\n",
    "# Output: ['large', 'language', 'model', 'revolutionizing', 'business', 'application']\n",
    "```\n",
    "\n",
    "## 3. Representing Text: Bag of Words and TF-IDF\n",
    "\n",
    "### Bag of Words (BoW)\n",
    "\n",
    "A simple way to represent text as numerical vectors by counting word occurrences:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Large language models revolutionize business.\",\n",
    "    \"Business applications benefit from AI.\",\n",
    "    \"Language models learn from text data.\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "# Output: ['ai', 'applications', 'benefit', 'business', 'data', 'from', 'language', 'large', 'learn', 'models', 'revolutionize', 'text']\n",
    "\n",
    "print(X.toarray())\n",
    "# Output: \n",
    "# [[0 0 0 1 0 0 1 1 0 1 1 0]\n",
    "#  [1 1 1 1 0 1 0 0 0 0 0 0]\n",
    "#  [0 0 0 0 1 1 1 0 1 1 0 1]]\n",
    "```\n",
    "\n",
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "A more sophisticated approach that weights terms based on their importance:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "# Same output as above\n",
    "\n",
    "print(X_tfidf.toarray())\n",
    "# Output will be a matrix of TF-IDF scores\n",
    "```\n",
    "\n",
    "## 4. Classical Language Models: N-grams\n",
    "\n",
    "N-gram models were among the earliest statistical language models, predicting the probability of a sequence of words by analyzing patterns in training data.\n",
    "\n",
    "### What are N-grams?\n",
    "\n",
    "N-grams are contiguous sequences of n items (words, characters, etc.) from a text:\n",
    "- **Unigrams**: Single words (e.g., \"language\")\n",
    "- **Bigrams**: Two consecutive words (e.g., \"language models\")\n",
    "- **Trigrams**: Three consecutive words (e.g., \"large language models\")\n",
    "\n",
    "### Implementing N-grams\n",
    "\n",
    "```python\n",
    "from nltk.util import ngrams\n",
    "\n",
    "text = \"Large language models are transforming how businesses operate\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Generate bigrams\n",
    "bigrams_list = list(ngrams(tokens, 2))\n",
    "print(bigrams_list)\n",
    "# Output: [('large', 'language'), ('language', 'models'), ('models', 'are'), ('are', 'transforming'), ('transforming', 'how'), ('how', 'businesses'), ('businesses', 'operate')]\n",
    "\n",
    "# Generate trigrams\n",
    "trigrams_list = list(ngrams(tokens, 3))\n",
    "print(trigrams_list)\n",
    "# Output: [('large', 'language', 'models'), ('language', 'models', 'are'), ('models', 'are', 'transforming'), ('are', 'transforming', 'how'), ('transforming', 'how', 'businesses'), ('how', 'businesses', 'operate')]\n",
    "```\n",
    "\n",
    "### Building a Simple N-gram Language Model\n",
    "\n",
    "```python\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_ngram_model(text, n=2):\n",
    "    \"\"\"Build an n-gram language model from text.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    ngrams_dict = defaultdict(list)\n",
    "    \n",
    "    # Create dictionary of n-grams and possible next words\n",
    "    for i in range(len(tokens) - n):\n",
    "        current_ngram = tuple(tokens[i:i+n])\n",
    "        next_word = tokens[i+n]\n",
    "        ngrams_dict[current_ngram].append(next_word)\n",
    "    \n",
    "    return ngrams_dict\n",
    "\n",
    "def generate_text(model, seed, length=20):\n",
    "    \"\"\"Generate text using the n-gram model.\"\"\"\n",
    "    current = seed\n",
    "    result = list(seed)\n",
    "    \n",
    "    for _ in range(length):\n",
    "        if current in model:\n",
    "            # Randomly select a possible next word\n",
    "            next_word = random.choice(model[current])\n",
    "            result.append(next_word)\n",
    "            # Update current n-gram\n",
    "            current = current[1:] + (next_word,)\n",
    "        else:\n",
    "            # If current n-gram is not in model, break\n",
    "            break\n",
    "    \n",
    "    return ' '.join(result)\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = \"\"\"Large language models are transforming how businesses operate. \n",
    "These models can understand language, generate text, and perform various tasks. \n",
    "Businesses use language models for customer service, content creation, and data analysis.\n",
    "Language models learn patterns from vast amounts of text data.\"\"\"\n",
    "\n",
    "# Build a bigram model\n",
    "bigram_model = build_ngram_model(corpus, 2)\n",
    "\n",
    "# Generate text using the model\n",
    "seed = ('language', 'models')\n",
    "generated_text = generate_text(bigram_model, seed)\n",
    "print(generated_text)\n",
    "# Output might be: \"language models learn patterns from vast amounts of text data businesses use language models for customer service content creation and data analysis\"\n",
    "```\n",
    "\n",
    "### Limitations of N-gram Models\n",
    "\n",
    "1. **Limited context**: Only consider a fixed number of previous words\n",
    "2. **Data sparsity**: Many valid word combinations don't appear in training data\n",
    "3. **No semantic understanding**: Model captures statistical patterns but not meaning\n",
    "4. **Memory intensive**: Storing all possible n-grams requires significant space\n",
    "5. **No long-range dependencies**: Cannot capture relationships between distant words\n",
    "\n",
    "## 5. Smoothing Techniques\n",
    "\n",
    "To address data sparsity in n-gram models, smoothing techniques redistribute probability mass to unseen events:\n",
    "\n",
    "### Laplace (Add-One) Smoothing\n",
    "\n",
    "```python\n",
    "def build_bigram_model_with_smoothing(text):\n",
    "    \"\"\"Build a bigram model with Laplace smoothing.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Count word frequencies\n",
    "    unigram_counts = defaultdict(int)\n",
    "    bigram_counts = defaultdict(int)\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        unigram_counts[tokens[i]] += 1\n",
    "        if i < len(tokens) - 1:\n",
    "            bigram = (tokens[i], tokens[i+1])\n",
    "            bigram_counts[bigram] += 1\n",
    "    \n",
    "    # Calculate probabilities with smoothing\n",
    "    vocab_size = len(unigram_counts)\n",
    "    bigram_model = {}\n",
    "    \n",
    "    for bigram, count in bigram_counts.items():\n",
    "        # P(w2|w1) = (count(w1,w2) + 1) / (count(w1) + V)\n",
    "        w1 = bigram[0]\n",
    "        bigram_model[bigram] = (count + 1) / (unigram_counts[w1] + vocab_size)\n",
    "    \n",
    "    return bigram_model, unigram_counts, vocab_size\n",
    "\n",
    "# Build model with smoothing\n",
    "model, unigram_counts, vocab_size = build_bigram_model_with_smoothing(corpus)\n",
    "\n",
    "# Calculate probability of a bigram\n",
    "def get_bigram_probability(model, unigram_counts, vocab_size, w1, w2):\n",
    "    \"\"\"Get the probability of a bigram with smoothing.\"\"\"\n",
    "    if (w1, w2) in model:\n",
    "        return model[(w1, w2)]\n",
    "    else:\n",
    "        # For unseen bigrams\n",
    "        return 1 / (unigram_counts[w1] + vocab_size)\n",
    "\n",
    "# Example probability\n",
    "prob = get_bigram_probability(model, unigram_counts, vocab_size, 'language', 'models')\n",
    "print(f\"P(models|language) = {prob:.4f}\")\n",
    "```\n",
    "\n",
    "## 6. Evaluating Language Models: Perplexity\n",
    "\n",
    "Perplexity measures how well a language model predicts a sample:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def calculate_perplexity(test_text, model, unigram_counts, vocab_size):\n",
    "    \"\"\"Calculate perplexity of test text using the bigram model.\"\"\"\n",
    "    tokens = word_tokenize(test_text.lower())\n",
    "    log_probability = 0\n",
    "    \n",
    "    for i in range(len(tokens) - 1):\n",
    "        bigram = (tokens[i], tokens[i+1])\n",
    "        probability = get_bigram_probability(model, unigram_counts, vocab_size, tokens[i], tokens[i+1])\n",
    "        log_probability += np.log2(probability)\n",
    "    \n",
    "    # Perplexity = 2^(-average log probability)\n",
    "    perplexity = 2 ** (-log_probability / (len(tokens) - 1))\n",
    "    return perplexity\n",
    "\n",
    "# Test the model on new text\n",
    "test_text = \"Language models help businesses understand customer feedback.\"\n",
    "perplexity = calculate_perplexity(test_text, model, unigram_counts, vocab_size)\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n",
    "```\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lecture, we've explored the foundations of NLP and classical language models, particularly focusing on n-gram models. While these approaches have significant limitations, they introduced important concepts that laid the groundwork for more advanced language models.\n",
    "\n",
    "In the next lecture, we'll examine the transition from statistical models to neural network-based approaches, including word embeddings and recurrent neural networks, which addressed many of the limitations of classical models.\n",
    "\n",
    "## Discussion Questions\n",
    "\n",
    "1. How might a business use basic NLP techniques like tokenization and sentiment analysis to gain insights from customer feedback?\n",
    "2. What are the key limitations of the bag-of-words approach for representing text?\n",
    "3. How could n-gram models be used in predictive text applications, and what challenges might arise?\n",
    "4. Why is context so important in language understanding, and how do classical models struggle with this?\n",
    "\n",
    "## Practical Assignment\n",
    "\n",
    "Implement a simple text classifier using the bag-of-words model and TF-IDF to categorize customer feedback as positive, negative, or neutral. Compare the performance of these two text representation approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|# Additional Language Challenges\n",
    "\n",
    "- **Context-dependent**: Meaning often depends on surrounding text or real-world knowledge\n",
    "  - **Lexical context**: The meaning of words depends on surrounding words\n",
    "    - Example: In \"She took the bat to the game\" vs. \"The bat flew into the cave,\" the meaning of \"bat\" changes completely\n",
    "    - Example: \"He was drawing a bow\" (musical instrument or weapon?) depends on previous sentences\n",
    "    - Example: \"She went to the bank\" (financial institution or riverbank?) requires surrounding context\n",
    "    - Computational challenge: Models must use nearby words to disambiguate meanings\n",
    "  \n",
    "  - **Discourse context**: The meaning depends on previous statements or conversation history\n",
    "    - Example: \"She likes it\" is meaningless without knowing what \"it\" refers to\n",
    "    - Example: \"That's not what I meant\" requires understanding previous utterances\n",
    "    - Example: \"This solution addresses the problem\" needs context about what problem is being discussed\n",
    "    - Computational challenge: Systems must track reference and topics across multiple sentences\n",
    "  \n",
    "  - **Domain knowledge**: The meaning requires specific background knowledge\n",
    "    - Example: Understanding \"He scored a perfect 300\" requires knowing if we're discussing bowling, SAT testing, or something else\n",
    "    - Example: \"The patient presents with elevated troponin levels\" requires medical knowledge\n",
    "    - Example: \"The forward executed a perfect pick and roll\" requires basketball knowledge\n",
    "    - Computational challenge: Models need domain-specific training or specialized knowledge bases\n",
    "  \n",
    "  - **World knowledge**: The meaning requires general understanding about how the world works\n",
    "    - Example: \"The coffee is too hot to drink\" implies waiting, while \"The coffee is too cold to drink\" might imply reheating\n",
    "    - Example: \"She couldn't fit the trophy in the suitcase because it was too big\" (what was too big? requires understanding size relationships)\n",
    "    - Example: \"The child couldn't reach the cookie jar on the shelf\" requires understanding physical capabilities\n",
    "    - Computational challenge: Models need broad common-sense reasoning capabilities\n",
    "\n",
    "- **Evolving**: Language changes over time with new words and usage patterns\n",
    "  - **Neologisms**: Creation of entirely new words\n",
    "    - Example: \"Doomscrolling,\" \"NFT,\" \"cryptocurrency,\" and \"webinar\" emerged in recent years\n",
    "    - Example: \"Unfriend,\" \"selfie,\" and \"binge-watch\" entered dictionaries in the past decade\n",
    "    - Example: Technology terms like \"blog,\" \"podcast,\" and \"smartphone\" that didn't exist 30 years ago\n",
    "    - Computational challenge: Models trained on older corpora lack these terms in their vocabulary\n",
    "  \n",
    "  - **Semantic shifts**: Existing words gaining new meanings\n",
    "    - Example: \"Cloud\" shifted from weather phenomenon to data storage\n",
    "    - Example: \"Tweet\" evolved from bird sound to social media post\n",
    "    - Example: \"Viral\" changed from referring to viruses to popular online content\n",
    "    - Computational challenge: Models must distinguish between traditional and newer meanings\n",
    "  \n",
    "  - **Functional shifts**: Words changing their grammatical roles\n",
    "    - Example: \"Google\" evolved from a proper noun to a verb (\"Let me google that\")\n",
    "    - Example: \"Adult\" transformed from adjective/noun to verb (\"I can't adult today\")\n",
    "    - Example: \"Friend\" changed from noun to verb (\"Friend me on Facebook\")\n",
    "    - Computational challenge: Part-of-speech tagging systems must accommodate these changes\n",
    "  \n",
    "  - **Semantic broadening/narrowing**: Words expanding or restricting their meaning\n",
    "    - Example: \"Sick\" expanded from purely negative (ill) to also positive (excellent)\n",
    "    - Example: \"Literally\" broadened to be used for emphasis, not just \"exactly as stated\"\n",
    "    - Example: \"Gay\" narrowed from \"happy\" to specifically referring to homosexuality\n",
    "    - Computational challenge: Systems must understand both historical and contemporary usage\n",
    "\n",
    "- **Culturally influenced**: Idioms, slang, and references vary across cultures\n",
    "  - **Idioms**: Expressions whose meanings cannot be derived from their individual words\n",
    "    - Example: \"Break a leg\" is encouragement in Western theater, not a violent wish\n",
    "    - Example: \"It's raining cats and dogs\" means heavy rainfall, not animals falling from the sky\n",
    "    - Example: \"Kick the bucket\" refers to dying, not striking a container with one's foot\n",
    "    - Computational challenge: Systems must learn thousands of idiomatic expressions as special cases\n",
    "  \n",
    "  - **Regional variations**: Differences in vocabulary and usage across regions\n",
    "    - Example: American English \"elevator\" vs. British English \"lift\"\n",
    "    - Example: American English \"first floor\" is ground level, while in British English it's one level up\n",
    "    - Example: \"Table a discussion\" means to postpone it in American English but to bring it forward in British English\n",
    "    - Computational challenge: Models must recognize dialect and adjust interpretation accordingly\n",
    "  \n",
    "  - **Cultural references**: Meanings that depend on shared cultural knowledge\n",
    "    - Example: Understanding \"May the Fourth be with you\" requires familiarity with Star Wars\n",
    "    - Example: \"Don't cross the streams\" is advice about avoiding disaster (from Ghostbusters)\n",
    "    - Example: \"Winter is coming\" may convey a sense of foreboding (from Game of Thrones)\n",
    "    - Computational challenge: Systems need massive cultural knowledge bases or must identify references\n",
    "  \n",
    "  - **Slang and colloquialisms**: Informal language that varies by social group and generation\n",
    "    - Example: \"That's sick\" meaning impressive or excellent among younger generations\n",
    "    - Example: \"Throwing shade\" meaning subtle criticism or disrespect\n",
    "    - Example: \"Ghost\" as a verb meaning to suddenly cut off communication\n",
    "    - Computational challenge: Slang evolves rapidly and varies widely across demographics\n",
    "\n",
    "- **Structurally complex**: Grammar, syntax, and semantics interact in sophisticated ways\n",
    "  - **Syntactic complexity**: Sentences with intricate grammatical structures\n",
    "    - Example: Garden path sentences that mislead the reader: \"The old man the boat\" (where \"man\" is a verb)\n",
    "    - Example: \"The horse raced past the barn fell\" (The horse that was raced past the barn fell)\n",
    "    - Example: Complex embeddings: \"The rat the cat the dog chased killed ate the cheese\"\n",
    "    - Computational challenge: Parsing requires handling multiple possible grammatical interpretations\n",
    "  \n",
    "  - **Anaphora resolution**: Determining what pronouns and references point to\n",
    "    - Example: \"John told Bill that he had won the lottery\" (Who won? John or Bill?)\n",
    "    - Example: \"The trophy wouldn't fit in the suitcase because it was too big\" (What was too big?)\n",
    "    - Example: \"After the doctors treated the patients, they went home\" (Who went home?)\n",
    "    - Computational challenge: Systems must identify the correct antecedent among multiple candidates\n",
    "  \n",
    "  - **Scope ambiguity**: Uncertainty about which parts of a sentence are affected by operators like negation\n",
    "    - Example: \"I did not go to the store because it was closed\" (Did I go to the store? The reason is ambiguous)\n",
    "    - Example: \"All students read one book\" (One specific book read by all, or different books for each student?)\n",
    "    - Example: \"She only likes blue shirts\" (Only likes blue shirts and nothing else? Or likes blue shirts but no other colors?)\n",
    "    - Computational challenge: Models must determine the intended scope of quantifiers and operators\n",
    "  \n",
    "  - **Long-distance dependencies**: Relationships between words that are far apart in a sentence\n",
    "    - Example: \"The document that the lawyer who the firm hired prepared was reviewed\"\n",
    "    - Example: \"What did you say you wanted to buy?\" (connecting \"what\" with \"buy\")\n",
    "    - Example: \"This is the house that Jack built that the malt lay in that the rat ate...\"\n",
    "    - Computational challenge: Systems must track relationships across many intervening words\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
