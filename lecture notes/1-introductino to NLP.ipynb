{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing and Classical Language Models\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lecture, students will be able to:\n",
    "1. Understand the basic concepts and challenges of Natural Language Processing\n",
    "2. Explain key text preprocessing techniques\n",
    "3. Describe and implement statistical language models\n",
    "4. Understand n-gram models and their limitations\n",
    "5. Apply basic NLP techniques using Python libraries\n",
    "\n",
    "## 1. Introduction to Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) is a field at the intersection of computer science, artificial intelligence, and linguistics focused on enabling computers to understand, interpret, and generate human language. It forms the foundation for all language technologies we use today, from search engines to voice assistants and modern large language models.\n",
    "\n",
    "### Why is NLP Challenging?\n",
    "\n",
    "Human language is:\n",
    "\n",
    "- **Ambiguous**: Words and sentences can have multiple interpretations\n",
    "  - **Lexical ambiguity**: When a word has multiple meanings\n",
    "    - Example: \"The bank is closed\" (financial institution or riverbank)\n",
    "    - Example: \"The match was struck\" (Small stick used to create fire or Competition between teams)\n",
    "    - Example: \"The suit was expensive\" (Formal set of clothes or Legal proceeding)\n",
    "    - Computational challenge: Systems must select the correct word sense from multiple possibilities\n",
    "  \n",
    "  - **Syntactic ambiguity**: When a sentence can be parsed in multiple ways\n",
    "    - Example: \"I saw the man with the telescope\" (Who has the telescope?)\n",
    "    - Example: \"Flying planes can be dangerous\" (The act of flying planes or planes that are flying?)\n",
    "    - Example: \"Time flies like an arrow\" (Multiple possible grammatical structures)\n",
    "    - Computational challenge: Models must determine the correct grammatical structure\n",
    "  \n",
    "  - **Semantic ambiguity**: When the meaning of a sentence has multiple interpretations\n",
    "    - Example: \"Every student took a different course\" (Each student took one course, and no two students took the same course? Or each student took multiple courses that differed from their own other courses?)\n",
    "    - Example: \"The chicken is ready to eat\" (Ready to be eaten or ready to consume food?)\n",
    "    - Example: \"John and Mary got married last year\" (To each other or to other people?)\n",
    "    - Computational challenge: Systems must infer the intended meaning based on context\n",
    "  \n",
    "  - **Pragmatic ambiguity**: When the intended meaning depends on context, intent, or implied information\n",
    "    - Example: \"It's cold in here\" (Statement of fact or request to close a window/turn up heat?)\n",
    "    - Example: \"Do you know what time it is?\" (Yes/no question or request for the time?)\n",
    "    - Example: \"Could you pass the salt?\" (Question about ability or request for action?)\n",
    "    - Computational challenge: Models must understand communicative intent beyond literal meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Techniques and Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP encompasses a wide range of techniques and applications that work together to process and understand language:\n",
    "\n",
    "#### Fundamental NLP Techniques\n",
    "\n",
    "1. **Tokenization**: Breaking text into words, phrases, or other meaningful elements\n",
    "   - Word tokenization: Splitting \"I love NLP.\" into [\"I\", \"love\", \"NLP\", \".\"]\n",
    "   - Subword tokenization: Breaking words into meaningful pieces (e.g., \"playing\" → [\"play\", \"##ing\"])\n",
    "   - Character tokenization: Splitting text into individual characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-LLMs/main/lecture%20notes/images/tokenization.png\" alt=\"Text Tokenization\" width=\"600\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Part-of-speech (POS) tagging**: Identifying whether words are nouns, verbs, adjectives, etc.\n",
    "   - \"The quick brown fox jumps\" → [Determiner, Adjective, Adjective, Noun, Verb]\n",
    "   - Critical for understanding grammatical structure and word meaning in context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-LLMs/main/lecture%20notes/images/POS-tagging.jpg\" alt=\"Text Tokenization\" width=\"600\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. **Named Entity Recognition (NER)**: Identifying proper nouns like people, organizations, locations\n",
    "   - \"Apple is releasing a new iPhone in San Francisco\" → [Organization, Product, Location]\n",
    "   - Useful for extracting key information from unstructured text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-LLMs/main/lecture%20notes/images/ner.png\" alt=\"Text Tokenization\" width=\"600\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Parsing**: Analyzing grammatical structure of sentences\n",
    "   - Constituency parsing: Constructing phrase structure trees\n",
    "   - Dependency parsing: Identifying grammatical relationships between words\n",
    "   - Semantic parsing: Mapping sentences to formal meaning representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-LLMs/main/lecture%20notes/images/parsing.png\" alt=\"Text Tokenization\" width=\"600\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Coreference resolution**: Determining when different words refer to the same entity\n",
    "   - \"John said he was tired\" → \"he\" refers to \"John\"\n",
    "   - Essential for understanding relationships across sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-LLMs/main/lecture%20notes/images/coreference.png\" alt=\"Text Tokenization\" width=\"600\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common NLP Applications\n",
    "\n",
    "1. **Sentiment analysis**: Determining the emotional tone of text\n",
    "   - Identifying if customer reviews are positive, negative, or neutral\n",
    "   - Tracking public opinion on products, services, or topics\n",
    "\n",
    "2. **Machine translation**: Translating text between languages\n",
    "   - Converting text from one language to another while preserving meaning\n",
    "   - Handling cultural and linguistic differences across languages\n",
    "\n",
    "3. **Text generation**: Creating human-like text\n",
    "   - Automatic summarization of longer documents\n",
    "   - Creative writing assistance and content creation\n",
    "   - Dialogue systems for conversational AI\n",
    "\n",
    "4. **Question answering**: Providing answers to natural language questions\n",
    "   - Extracting answers from documents or knowledge bases\n",
    "   - Understanding user intent and providing relevant information\n",
    "\n",
    "5. **Information extraction**: Identifying and extracting structured information from text\n",
    "   - Pulling key data points from unstructured documents\n",
    "   - Converting text documents into structured databases\n",
    "\n",
    "6. **Text classification**: Categorizing text into predefined categories\n",
    "   - Sorting emails into spam/not spam\n",
    "   - Organizing documents by topic or content type\n",
    "\n",
    "Understanding these techniques and applications provides the foundation for studying language models, which combine these elements to create systems that can comprehend and generate human language at increasingly sophisticated levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n",
    "Before building language models, we need to prepare and clean text data through several preprocessing steps:\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Splitting text into meaningful units (typically words or subwords)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing NLTK\n",
    "\n",
    "NLTK (Natural Language Toolkit) is a leading platform for building Python programs that work with human language data. Developed by Steven Bird and Edward Loper at the University of Pennsylvania, NLTK has become one of the most widely used libraries for natural language processing (NLP) in Python since its initial release in 2001.\n",
    "Key Features of NLTK\n",
    "\n",
    "Comprehensive Toolkit: NLTK provides easy-to-use interfaces to over 50 corpora and lexical resources, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
    "Educational Purpose: NLTK was designed with education in mind. It comes with comprehensive documentation and a book (\"Natural Language Processing with Python\") that makes it accessible for beginners.\n",
    "Built-in Datasets: NLTK includes various pre-loaded corpora like the Brown Corpus, WordNet, and many others that are useful for training and testing NLP algorithms.\n",
    "Modular Architecture: The library is organized into modules focused on specific NLP tasks, making it easy to use just the components you need.\n",
    "\n",
    "**Core Functionality**\n",
    "NLTK supports many essential NLP tasks:\n",
    "\n",
    "- Tokenization: Breaking text into words, sentences, or other meaningful elements\n",
    "- Part-of-speech tagging: Identifying the grammatical parts of speech for words\n",
    "- Named entity recognition: Identifying names of people, organizations, locations, etc.\n",
    "- Stemming and lemmatization: Reducing words to their base or root form\n",
    "- Parsing: Analyzing syntactic structures of sentences\n",
    "- Semantic analysis: Working with meaning representations\n",
    "- Sentiment analysis: Determining positive, negative, or neutral sentiment in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the require packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "# install NLTK package\n",
    "! pip install nltk\n",
    "\n",
    "# install NLTK data\n",
    "! python -m nltk.downloader all\n",
    "\n",
    "# install punkt package\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Show where nltk is looking for the data\n",
    "import nltk\n",
    "print(nltk.data.path)  # Shows where NLTK is looking for data\n",
    "\n",
    "# Look for the nltk_data folder and set its path\n",
    "import nltk\n",
    "nltk.data.path.append(\"C:\\\\Users\\\\me\\\\AppData\\\\Roaming\\\\nltk_data\\\\tokenizer\")  # Add the path to the data folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization example\n",
    "The simple split method can't handle the punctuations correctly i.e. applications.<br>\n",
    "The NLTK package has different advanced features for tokenization text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Large', 'language', 'models', 'are', 'revolutionizing', 'business', 'applications.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\me/nltk_data'\n    - 'c:\\\\Users\\\\me\\\\myenv\\\\nltk_data'\n    - 'c:\\\\Users\\\\me\\\\myenv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\me\\\\myenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\me\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\me\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\me\\\\AppData\\\\Roaming\\\\nltk_data\\\\'\n    - 'C:\\\\Users\\\\me\\\\AppData\\\\Roaming\\\\nltk_data\\\\tokenizer'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mpunkt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokens)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Output: ['Large', 'language', 'models', 'are', 'revolutionizing', 'business', 'applications', '.']\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\me\\myenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\me\\myenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\me\\myenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\me\\myenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\me\\myenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\me\\myenv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\me/nltk_data'\n    - 'c:\\\\Users\\\\me\\\\myenv\\\\nltk_data'\n    - 'c:\\\\Users\\\\me\\\\myenv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\me\\\\myenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\me\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\me\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\me\\\\AppData\\\\Roaming\\\\nltk_data\\\\'\n    - 'C:\\\\Users\\\\me\\\\AppData\\\\Roaming\\\\nltk_data\\\\tokenizer'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Simple word tokenization\n",
    "sentence = \"Large language models are revolutionizing business applications.\"\n",
    "tokens = sentence.split()\n",
    "print(tokens)\n",
    "# Output: ['Large', 'language', 'models', 'are', 'revolutionizing', 'business', 'applications.']\n",
    "\n",
    "# Using NLTK for more sophisticated tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "# Output: ['Large', 'language', 'models', 'are', 'revolutionizing', 'business', 'applications', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Converting text to a standard form to reduce variability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing\n",
    "normalized_tokens = [token.lower() for token in tokens]\n",
    "print(normalized_tokens)\n",
    "# Output: ['large', 'language', 'models', 'are', 'revolutionizing', 'business', 'applications', '.']\n",
    "\n",
    "# Removing punctuation\n",
    "import re\n",
    "normalized_tokens = [re.sub(r'[^\\w\\s]', '', token.lower()) for token in tokens]\n",
    "print(normalized_tokens)\n",
    "# Output: ['large', 'language', 'models', 'are', 'revolutionizing', 'business', 'applications', '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "\n",
    "Eliminating common words that add little meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in normalized_tokens if token and token not in stop_words]\n",
    "print(filtered_tokens)\n",
    "# Output: ['large', 'language', 'models', 'revolutionizing', 'business', 'applications']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "Reducing words to their root forms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(stemmed_tokens)\n",
    "# Output: ['larg', 'languag', 'model', 'revolution', 'busi', 'applic']\n",
    "\n",
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print(lemmatized_tokens)\n",
    "# Output: ['large', 'language', 'model', 'revolutionizing', 'business', 'application']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Representing Text: Bag of Words and TF-IDF\n",
    "\n",
    "### Bag of Words (BoW)\n",
    "\n",
    "A simple way to represent text as numerical vectors by counting word occurrences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Large language models revolutionize business.\",\n",
    "    \"Business applications benefit from AI.\",\n",
    "    \"Language models learn from text data.\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "# Output: ['ai', 'applications', 'benefit', 'business', 'data', 'from', 'language', 'large', 'learn', 'models', 'revolutionize', 'text']\n",
    "\n",
    "print(X.toarray())\n",
    "# Output: \n",
    "# [[0 0 0 1 0 0 1 1 0 1 1 0]\n",
    "#  [1 1 1 1 0 1 0 0 0 0 0 0]\n",
    "#  [0 0 0 0 1 1 1 0 1 1 0 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "A more sophisticated approach that weights terms based on their importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "# Same output as above\n",
    "\n",
    "print(X_tfidf.toarray())\n",
    "# Output will be a matrix of TF-IDF scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classical Language Models: N-grams\n",
    "\n",
    "N-gram models were among the earliest statistical language models, predicting the probability of a sequence of words by analyzing patterns in training data.\n",
    "\n",
    "### What are N-grams?\n",
    "\n",
    "N-grams are contiguous sequences of n items (words, characters, etc.) from a text:\n",
    "- **Unigrams**: Single words (e.g., \"language\")\n",
    "- **Bigrams**: Two consecutive words (e.g., \"language models\")\n",
    "- **Trigrams**: Three consecutive words (e.g., \"large language models\")\n",
    "\n",
    "### Implementing N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "text = \"Large language models are transforming how businesses operate\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Generate bigrams\n",
    "bigrams_list = list(ngrams(tokens, 2))\n",
    "print(bigrams_list)\n",
    "# Output: [('large', 'language'), ('language', 'models'), ('models', 'are'), ('are', 'transforming'), ('transforming', 'how'), ('how', 'businesses'), ('businesses', 'operate')]\n",
    "\n",
    "# Generate trigrams\n",
    "trigrams_list = list(ngrams(tokens, 3))\n",
    "print(trigrams_list)\n",
    "# Output: [('large', 'language', 'models'), ('language', 'models', 'are'), ('models', 'are', 'transforming'), ('are', 'transforming', 'how'), ('transforming', 'how', 'businesses'), ('how', 'businesses', 'operate')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Simple N-gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_ngram_model(text, n=2):\n",
    "    \"\"\"Build an n-gram language model from text.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    ngrams_dict = defaultdict(list)\n",
    "    \n",
    "    # Create dictionary of n-grams and possible next words\n",
    "    for i in range(len(tokens) - n):\n",
    "        current_ngram = tuple(tokens[i:i+n])\n",
    "        next_word = tokens[i+n]\n",
    "        ngrams_dict[current_ngram].append(next_word)\n",
    "    \n",
    "    return ngrams_dict\n",
    "\n",
    "def generate_text(model, seed, length=20):\n",
    "    \"\"\"Generate text using the n-gram model.\"\"\"\n",
    "    current = seed\n",
    "    result = list(seed)\n",
    "    \n",
    "    for _ in range(length):\n",
    "        if current in model:\n",
    "            # Randomly select a possible next word\n",
    "            next_word = random.choice(model[current])\n",
    "            result.append(next_word)\n",
    "            # Update current n-gram\n",
    "            current = current[1:] + (next_word,)\n",
    "        else:\n",
    "            # If current n-gram is not in model, break\n",
    "            break\n",
    "    \n",
    "    return ' '.join(result)\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = \"\"\"Large language models are transforming how businesses operate. \n",
    "These models can understand language, generate text, and perform various tasks. \n",
    "Businesses use language models for customer service, content creation, and data analysis.\n",
    "Language models learn patterns from vast amounts of text data.\"\"\"\n",
    "\n",
    "# Build a bigram model\n",
    "bigram_model = build_ngram_model(corpus, 2)\n",
    "\n",
    "# Generate text using the model\n",
    "seed = ('language', 'models')\n",
    "generated_text = generate_text(bigram_model, seed)\n",
    "print(generated_text)\n",
    "# Output might be: \"language models learn patterns from vast amounts of text data businesses use language models for customer service content creation and data analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of N-gram Models\n",
    "\n",
    "1. **Limited context**: Only consider a fixed number of previous words\n",
    "2. **Data sparsity**: Many valid word combinations don't appear in training data\n",
    "3. **No semantic understanding**: Model captures statistical patterns but not meaning\n",
    "4. **Memory intensive**: Storing all possible n-grams requires significant space\n",
    "5. **No long-range dependencies**: Cannot capture relationships between distant words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Smoothing Techniques\n",
    "\n",
    "To address data sparsity in n-gram models, smoothing techniques redistribute probability mass to unseen events:\n",
    "\n",
    "### Laplace (Add-One) Smoothin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram_model_with_smoothing(text):\n",
    "    \"\"\"Build a bigram model with Laplace smoothing.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Count word frequencies\n",
    "    unigram_counts = defaultdict(int)\n",
    "    bigram_counts = defaultdict(int)\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        unigram_counts[tokens[i]] += 1\n",
    "        if i < len(tokens) - 1:\n",
    "            bigram = (tokens[i], tokens[i+1])\n",
    "            bigram_counts[bigram] += 1\n",
    "    \n",
    "    # Calculate probabilities with smoothing\n",
    "    vocab_size = len(unigram_counts)\n",
    "    bigram_model = {}\n",
    "    \n",
    "    for bigram, count in bigram_counts.items():\n",
    "        # P(w2|w1) = (count(w1,w2) + 1) / (count(w1) + V)\n",
    "        w1 = bigram[0]\n",
    "        bigram_model[bigram] = (count + 1) / (unigram_counts[w1] + vocab_size)\n",
    "    \n",
    "    return bigram_model, unigram_counts, vocab_size\n",
    "\n",
    "# Build model with smoothing\n",
    "model, unigram_counts, vocab_size = build_bigram_model_with_smoothing(corpus)\n",
    "\n",
    "# Calculate probability of a bigram\n",
    "def get_bigram_probability(model, unigram_counts, vocab_size, w1, w2):\n",
    "    \"\"\"Get the probability of a bigram with smoothing.\"\"\"\n",
    "    if (w1, w2) in model:\n",
    "        return model[(w1, w2)]\n",
    "    else:\n",
    "        # For unseen bigrams\n",
    "        return 1 / (unigram_counts[w1] + vocab_size)\n",
    "\n",
    "# Example probability\n",
    "prob = get_bigram_probability(model, unigram_counts, vocab_size, 'language', 'models')\n",
    "print(f\"P(models|language) = {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating Language Models: Perplexity\n",
    "Perplexity measures how well a language model predicts a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_perplexity(test_text, model, unigram_counts, vocab_size):\n",
    "    \"\"\"Calculate perplexity of test text using the bigram model.\"\"\"\n",
    "    tokens = word_tokenize(test_text.lower())\n",
    "    log_probability = 0\n",
    "    \n",
    "    for i in range(len(tokens) - 1):\n",
    "        bigram = (tokens[i], tokens[i+1])\n",
    "        probability = get_bigram_probability(model, unigram_counts, vocab_size, tokens[i], tokens[i+1])\n",
    "        log_probability += np.log2(probability)\n",
    "    \n",
    "    # Perplexity = 2^(-average log probability)\n",
    "    perplexity = 2 ** (-log_probability / (len(tokens) - 1))\n",
    "    return perplexity\n",
    "\n",
    "# Test the model on new text\n",
    "test_text = \"Language models help businesses understand customer feedback.\"\n",
    "perplexity = calculate_perplexity(test_text, model, unigram_counts, vocab_size)\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lecture, we've explored the foundations of NLP and classical language models, particularly focusing on n-gram models. While these approaches have significant limitations, they introduced important concepts that laid the groundwork for more advanced language models.\n",
    "\n",
    "In the next lecture, we'll examine the transition from statistical models to neural network-based approaches, including word embeddings and recurrent neural networks, which addressed many of the limitations of classical models.\n",
    "\n",
    "## Discussion Questions\n",
    "\n",
    "1. How might a business use basic NLP techniques like tokenization and sentiment analysis to gain insights from customer feedback?\n",
    "2. What are the key limitations of the bag-of-words approach for representing text?\n",
    "3. How could n-gram models be used in predictive text applications, and what challenges might arise?\n",
    "4. Why is context so important in language understanding, and how do classical models struggle with this?\n",
    "\n",
    "## Practical Assignment\n",
    "\n",
    "Implement a simple text classifier using the bag-of-words model and TF-IDF to categorize customer feedback as positive, negative, or neutral. Compare the performance of these two text representation approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Language Challenges\n",
    "غير مطلوب - مطالعة ذاتية\n",
    "\n",
    "- **Context-dependent**: Meaning often depends on surrounding text or real-world knowledge\n",
    "  - **Lexical context**: The meaning of words depends on surrounding words\n",
    "    - Example: In \"She took the bat to the game\" vs. \"The bat flew into the cave,\" the meaning of \"bat\" changes completely\n",
    "    - Example: \"He was drawing a bow\" (musical instrument or weapon?) depends on previous sentences\n",
    "    - Example: \"She went to the bank\" (financial institution or riverbank?) requires surrounding context\n",
    "    - Computational challenge: Models must use nearby words to disambiguate meanings\n",
    "  \n",
    "  - **Discourse context**: The meaning depends on previous statements or conversation history\n",
    "    - Example: \"She likes it\" is meaningless without knowing what \"it\" refers to\n",
    "    - Example: \"That's not what I meant\" requires understanding previous utterances\n",
    "    - Example: \"This solution addresses the problem\" needs context about what problem is being discussed\n",
    "    - Computational challenge: Systems must track reference and topics across multiple sentences\n",
    "  \n",
    "  - **Domain knowledge**: The meaning requires specific background knowledge\n",
    "    - Example: Understanding \"He scored a perfect 300\" requires knowing if we're discussing bowling, SAT testing, or something else\n",
    "    - Example: \"The patient presents with elevated troponin levels\" requires medical knowledge\n",
    "    - Example: \"The forward executed a perfect pick and roll\" requires basketball knowledge\n",
    "    - Computational challenge: Models need domain-specific training or specialized knowledge bases\n",
    "  \n",
    "  - **World knowledge**: The meaning requires general understanding about how the world works\n",
    "    - Example: \"The coffee is too hot to drink\" implies waiting, while \"The coffee is too cold to drink\" might imply reheating\n",
    "    - Example: \"She couldn't fit the trophy in the suitcase because it was too big\" (what was too big? requires understanding size relationships)\n",
    "    - Example: \"The child couldn't reach the cookie jar on the shelf\" requires understanding physical capabilities\n",
    "    - Computational challenge: Models need broad common-sense reasoning capabilities\n",
    "\n",
    "- **Evolving**: Language changes over time with new words and usage patterns\n",
    "  - **Neologisms**: Creation of entirely new words\n",
    "    - Example: \"Doomscrolling,\" \"NFT,\" \"cryptocurrency,\" and \"webinar\" emerged in recent years\n",
    "    - Example: \"Unfriend,\" \"selfie,\" and \"binge-watch\" entered dictionaries in the past decade\n",
    "    - Example: Technology terms like \"blog,\" \"podcast,\" and \"smartphone\" that didn't exist 30 years ago\n",
    "    - Computational challenge: Models trained on older corpora lack these terms in their vocabulary\n",
    "  \n",
    "  - **Semantic shifts**: Existing words gaining new meanings\n",
    "    - Example: \"Cloud\" shifted from weather phenomenon to data storage\n",
    "    - Example: \"Tweet\" evolved from bird sound to social media post\n",
    "    - Example: \"Viral\" changed from referring to viruses to popular online content\n",
    "    - Computational challenge: Models must distinguish between traditional and newer meanings\n",
    "  \n",
    "  - **Functional shifts**: Words changing their grammatical roles\n",
    "    - Example: \"Google\" evolved from a proper noun to a verb (\"Let me google that\")\n",
    "    - Example: \"Adult\" transformed from adjective/noun to verb (\"I can't adult today\")\n",
    "    - Example: \"Friend\" changed from noun to verb (\"Friend me on Facebook\")\n",
    "    - Computational challenge: Part-of-speech tagging systems must accommodate these changes\n",
    "  \n",
    "  - **Semantic broadening/narrowing**: Words expanding or restricting their meaning\n",
    "    - Example: \"Sick\" expanded from purely negative (ill) to also positive (excellent)\n",
    "    - Example: \"Literally\" broadened to be used for emphasis, not just \"exactly as stated\"\n",
    "    - Example: \"Gay\" narrowed from \"happy\" to specifically referring to homosexuality\n",
    "    - Computational challenge: Systems must understand both historical and contemporary usage\n",
    "\n",
    "- **Culturally influenced**: Idioms, slang, and references vary across cultures\n",
    "  - **Idioms**: Expressions whose meanings cannot be derived from their individual words\n",
    "    - Example: \"Break a leg\" is encouragement in Western theater, not a violent wish\n",
    "    - Example: \"It's raining cats and dogs\" means heavy rainfall, not animals falling from the sky\n",
    "    - Example: \"Kick the bucket\" refers to dying, not striking a container with one's foot\n",
    "    - Computational challenge: Systems must learn thousands of idiomatic expressions as special cases\n",
    "  \n",
    "  - **Regional variations**: Differences in vocabulary and usage across regions\n",
    "    - Example: American English \"elevator\" vs. British English \"lift\"\n",
    "    - Example: American English \"first floor\" is ground level, while in British English it's one level up\n",
    "    - Example: \"Table a discussion\" means to postpone it in American English but to bring it forward in British English\n",
    "    - Computational challenge: Models must recognize dialect and adjust interpretation accordingly\n",
    "  \n",
    "  - **Cultural references**: Meanings that depend on shared cultural knowledge\n",
    "    - Example: Understanding \"May the Fourth be with you\" requires familiarity with Star Wars\n",
    "    - Example: \"Don't cross the streams\" is advice about avoiding disaster (from Ghostbusters)\n",
    "    - Example: \"Winter is coming\" may convey a sense of foreboding (from Game of Thrones)\n",
    "    - Computational challenge: Systems need massive cultural knowledge bases or must identify references\n",
    "  \n",
    "  - **Slang and colloquialisms**: Informal language that varies by social group and generation\n",
    "    - Example: \"That's sick\" meaning impressive or excellent among younger generations\n",
    "    - Example: \"Throwing shade\" meaning subtle criticism or disrespect\n",
    "    - Example: \"Ghost\" as a verb meaning to suddenly cut off communication\n",
    "    - Computational challenge: Slang evolves rapidly and varies widely across demographics\n",
    "\n",
    "- **Structurally complex**: Grammar, syntax, and semantics interact in sophisticated ways\n",
    "  - **Syntactic complexity**: Sentences with intricate grammatical structures\n",
    "    - Example: Garden path sentences that mislead the reader: \"The old man the boat\" (where \"man\" is a verb)\n",
    "    - Example: \"The horse raced past the barn fell\" (The horse that was raced past the barn fell)\n",
    "    - Example: Complex embeddings: \"The rat the cat the dog chased killed ate the cheese\"\n",
    "    - Computational challenge: Parsing requires handling multiple possible grammatical interpretations\n",
    "  \n",
    "  - **Anaphora resolution**: Determining what pronouns and references point to\n",
    "    - Example: \"John told Bill that he had won the lottery\" (Who won? John or Bill?)\n",
    "    - Example: \"The trophy wouldn't fit in the suitcase because it was too big\" (What was too big?)\n",
    "    - Example: \"After the doctors treated the patients, they went home\" (Who went home?)\n",
    "    - Computational challenge: Systems must identify the correct antecedent among multiple candidates\n",
    "  \n",
    "  - **Scope ambiguity**: Uncertainty about which parts of a sentence are affected by operators like negation\n",
    "    - Example: \"I did not go to the store because it was closed\" (Did I go to the store? The reason is ambiguous)\n",
    "    - Example: \"All students read one book\" (One specific book read by all, or different books for each student?)\n",
    "    - Example: \"She only likes blue shirts\" (Only likes blue shirts and nothing else? Or likes blue shirts but no other colors?)\n",
    "    - Computational challenge: Models must determine the intended scope of quantifiers and operators\n",
    "  \n",
    "  - **Long-distance dependencies**: Relationships between words that are far apart in a sentence\n",
    "    - Example: \"The document that the lawyer who the firm hired prepared was reviewed\"\n",
    "    - Example: \"What did you say you wanted to buy?\" (connecting \"what\" with \"buy\")\n",
    "    - Example: \"This is the house that Jack built that the malt lay in that the rat ate...\"\n",
    "    - Computational challenge: Systems must track relationships across many intervening words\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
